{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   \tID          \tSIZE  \tMODIFIED     \n",
      "nomic-embed-text:latest\t0a109f422b47\t274 MB\t11 days ago \t\n",
      "llama3:8b              \t365c0bd3c000\t4.7 GB\t2 weeks ago \t\n",
      "llama3.1:8b            \t75382d0899df\t4.7 GB\t3 weeks ago \t\n",
      "llama3:text            \t870a5d02cfaf\t4.7 GB\t3 weeks ago \t\n",
      "qwen2:7b               \te0d4e1163c58\t4.4 GB\t3 weeks ago \t\n",
      "gemma2:9b              \tff02c3702f32\t5.4 GB\t3 weeks ago \t\n",
      "mistral:latest         \t2ae6f6dd7a3d\t4.1 GB\t2 months ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = \"llama3:8b\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3:8b')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_abstractive(text, llm):\n",
    "    prompt = f\"\"\"Write a summary in a narrative style that presents the content in a continuous, flowing manner, focusing on a chronological order of events. The narrative should be smooth without abrupt breaks, covering all relevant points comprehensively, but without delving into deep, specific details. The content should provide a clear overview of the entire event or story to give readers a broad understanding.\n",
    "    Please read the following news article carefully and follow the instructions below\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Instructions:\n",
    "    - Write each paragraph based on the sequence of events or background, focusing on a continuous storytelling approach.\n",
    "    - Use \\t at the beginning of each paragraph to create indentation.\n",
    "    - DO NOT leave blank lines between paragraphs. All paragraphs must be continuous with no blank lines.\n",
    "    - There is NO need to give titles for each paragraph. Focus on telling the story while covering key points.\n",
    "\n",
    "    IMPORTANT: All your answers must be in Thai. Ensure that the Thai translation accurately conveys the meaning and context of the original content.\"\"\"\n",
    "\n",
    "    system_template = f\"\"\"You are an expert summarizer with the ability to distill complex information into concise and accurate summaries. Your summaries are clear, coherent, and excellent at identifying key points while preserving the essence of the original text.\"\"\"\n",
    "\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "        system_template)\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(prompt)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "    messages = chat_prompt.format_messages(text=text)\n",
    "\n",
    "    response = llm(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_extractive(text, llm):\n",
    "    prompt = f\"\"\"Create a summary by selecting and combining key sentences directly from the original text. The summary should present the content in a chronological order, covering all relevant points comprehensively, but without adding any new words or phrases not present in the original text. The extracted sentences should provide a clear overview of the entire event or story to give readers a broad understanding.\n",
    "    Please read the following news article carefully and follow the instructions below:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Instructions:\n",
    "    - Use \\t at the beginning of each paragraph to create indentation.\n",
    "    - DO NOT leave blank lines between paragraphs. All paragraphs must be continuous with no blank lines.\n",
    "    - DO NOT add any new words or phrases. Use ONLY the exact text from the original article.\n",
    "    - There is NO need to give titles for each paragraph. Focus on presenting the key information while maintaining the original wording.\n",
    "    - Please DO NOT generate anything else, generate ONLY the summary.\n",
    "\n",
    "    IMPORTANT: Ensure that you only extract and combine existing sentences from the Thai text. Do not translate or rephrase any content.\"\"\"\n",
    "\n",
    "    system_template = f\"\"\"You are an expert in extractive summarization with the ability to identify and select the most important sentences from complex texts. Your summaries are coherent compilations of original sentences that accurately represent the key points and overall narrative of the source material.\"\"\"\n",
    "\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(prompt)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "    messages = chat_prompt.format_messages(text=text)\n",
    "\n",
    "    response = llm(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_sum = pd.read_csv(\n",
    "    r'F:\\AI\\Super AI SS4\\Level 3 - INTERN\\Dataset\\ThEconSum\\AIFORTHAI-TextSummarizationCorpus\\ThEconSum-random-10-10.csv')\n",
    "\n",
    "df_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum = pd.read_csv(\n",
    "    r'F:\\AI\\Super AI SS4\\Level 3 - INTERN\\Jupyter Notebook\\Evaluate\\llama3-ThEconSum-20.csv')\n",
    "\n",
    "df_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\Nitro 5\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "100%|██████████| 10/10 [42:46<00:00, 256.68s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df_sum['sum_abstractive'] = df_sum['content'].progress_apply(\n",
    "    lambda x: summarize_abstractive(x, llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [35:11<00:00, 105.60s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df_sum['sum_extractive'] = df_sum['content'].progress_apply(\n",
    "    lambda x: summarize_extractive(x, llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_sum.to_csv(\n",
    "#     r'F:\\AI\\Super AI SS4\\Level 3 - INTERN\\Jupyter Notebook\\Evaluate\\llama3-ThEconSum-random-10-10-ab.csv')\n",
    "# df_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum = pd.read_csv(\n",
    "    r'F:\\AI\\Super AI SS4\\Level 3 - INTERN\\Jupyter Notebook\\Evaluate\\llama3-ThEconSum-20.csv')\n",
    "\n",
    "df_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content', 'abstractive', 'sum_abstractive'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = df_sum[['content', 'abstractive', 'sum_abstractive']].copy()\n",
    "df_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_bert_score(candidate, reference):\n",
    "    P, R, F1 = score([candidate], [reference], lang='th', verbose=False)\n",
    "    bert_score = {\n",
    "        'precision': float(P[0]),\n",
    "        'recall': float(R[0]),\n",
    "        'f1': float(F1[0])\n",
    "    }\n",
    "    return bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT Scores ...: 100%|██████████| 20/20 [01:44<00:00,  5.22s/it]\n"
     ]
    }
   ],
   "source": [
    "bert_precisions = []\n",
    "bert_recalls = []\n",
    "bert_f1_scores = []\n",
    "\n",
    "for index, row in tqdm(df_copy.iterrows(), total=df_copy.shape[0], desc=\"BERT Scores ...\"):\n",
    "    candidate = row['sum_extractive']\n",
    "    reference = row['extractive']\n",
    "\n",
    "    bert_score = calculate_bert_score(candidate, reference)\n",
    "\n",
    "    bert_precisions.append(bert_score['precision'])\n",
    "    bert_recalls.append(bert_score['recall'])\n",
    "    bert_f1_scores.append(bert_score['f1'])\n",
    "\n",
    "df_copy['BERT_Precision'] = bert_precisions\n",
    "df_copy['BERT_Recall'] = bert_recalls\n",
    "df_copy['BERT_F1'] = bert_f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores_for_row(candidate, reference, engine=None):\n",
    "    if engine:\n",
    "        hypothesis_tokenized = ' '.join(word_tokenize(candidate, engine=engine))\n",
    "        reference_tokenized = ' '.join(word_tokenize(reference, engine=engine))\n",
    "    else:  # ใช้ default\n",
    "        hypothesis_tokenized = ' '.join(word_tokenize(candidate))\n",
    "        reference_tokenized = ' '.join(word_tokenize(reference))\n",
    "\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis_tokenized, reference_tokenized)[0]\n",
    "\n",
    "    rouge1_f1 = scores['rouge-1']['f']\n",
    "    rouge2_f1 = scores['rouge-2']['f']\n",
    "    rougel_f1 = scores['rouge-l']['f']\n",
    "\n",
    "    avg_rouge_score = (rouge1_f1 + rouge2_f1 + rougel_f1) / 3\n",
    "\n",
    "    return rouge1_f1, rouge2_f1, rougel_f1, avg_rouge_score\n",
    "\n",
    "def calculate_update_dataframe_ab(df, engine=None):\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougel_scores = []\n",
    "    rouge_avg_scores = []\n",
    "\n",
    "    engine_label = engine if engine else 'default'\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Calculating ROUGE Scores with {engine_label}\"):\n",
    "        candidate = row['sum_abstractive']\n",
    "        reference = row['abstractive']\n",
    "\n",
    "        rouge1_f1, rouge2_f1, rougel_f1, avg_rouge_score = calculate_rouge_scores_for_row(candidate, reference, engine=engine)\n",
    "\n",
    "        rouge1_scores.append(rouge1_f1)\n",
    "        rouge2_scores.append(rouge2_f1)\n",
    "        rougel_scores.append(rougel_f1)\n",
    "        rouge_avg_scores.append(avg_rouge_score)\n",
    "\n",
    "    df[f'ROUGE-1_{engine_label}'] = rouge1_scores\n",
    "    df[f'ROUGE-2_{engine_label}'] = rouge2_scores\n",
    "    df[f'ROUGE-L_{engine_label}'] = rougel_scores\n",
    "    df[f'ROUGE_Score_{engine_label}'] = rouge_avg_scores\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_update_dataframe_ex(df, engine=None):\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougel_scores = []\n",
    "    rouge_avg_scores = []\n",
    "\n",
    "    engine_label = engine if engine else 'default'\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Calculating ROUGE Scores with {engine_label}\"):\n",
    "        candidate = row['sum_extractive']\n",
    "        reference = row['extractive']\n",
    "\n",
    "        rouge1_f1, rouge2_f1, rougel_f1, avg_rouge_score = calculate_rouge_scores_for_row(candidate, reference, engine=engine)\n",
    "\n",
    "        rouge1_scores.append(rouge1_f1)\n",
    "        rouge2_scores.append(rouge2_f1)\n",
    "        rougel_scores.append(rougel_f1)\n",
    "        rouge_avg_scores.append(avg_rouge_score)\n",
    "\n",
    "    df[f'ROUGE-1_{engine_label}'] = rouge1_scores\n",
    "    df[f'ROUGE-2_{engine_label}'] = rouge2_scores\n",
    "    df[f'ROUGE-L_{engine_label}'] = rougel_scores\n",
    "    df[f'ROUGE_Score_{engine_label}'] = rouge_avg_scores\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE Scores with attacut: 100%|██████████| 20/20 [00:01<00:00, 10.02it/s]\n",
      "Calculating ROUGE Scores with default: 100%|██████████| 20/20 [00:00<00:00, 21.61it/s]\n"
     ]
    }
   ],
   "source": [
    "df_copy_attacut = df_copy.copy()\n",
    "df_copy_default = df_copy.copy()\n",
    "\n",
    "df_copy_attacut = calculate_update_dataframe_ab(df_copy_attacut, engine='attacut')\n",
    "df_copy_default = calculate_update_dataframe_ab(df_copy_default)  # ใช้ default\n",
    "\n",
    "avg_attacut = df_copy_attacut[[f'ROUGE-1_attacut', f'ROUGE-2_attacut', f'ROUGE-L_attacut', f'ROUGE_Score_attacut']].mean()\n",
    "avg_default = df_copy_default[[f'ROUGE-1_default', f'ROUGE-2_default', f'ROUGE-L_default', f'ROUGE_Score_default']].mean()\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE_Score'],\n",
    "    'Average_Attacut': avg_attacut.values,\n",
    "    'Average_Default': avg_default.values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Average_Attacut</th>\n",
       "      <th>Average_Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROUGE-1</td>\n",
       "      <td>0.655768</td>\n",
       "      <td>0.611001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROUGE-2</td>\n",
       "      <td>0.422972</td>\n",
       "      <td>0.404282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROUGE-L</td>\n",
       "      <td>0.614958</td>\n",
       "      <td>0.572372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ROUGE_Score</td>\n",
       "      <td>0.564566</td>\n",
       "      <td>0.529218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Metric  Average_Attacut  Average_Default\n",
       "0      ROUGE-1         0.655768         0.611001\n",
       "1      ROUGE-2         0.422972         0.404282\n",
       "2      ROUGE-L         0.614958         0.572372\n",
       "3  ROUGE_Score         0.564566         0.529218"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content', 'extractive', 'sum_extractive'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = df_sum[['content', 'extractive', 'sum_extractive']].copy()\n",
    "df_copy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE Scores with attacut: 100%|██████████| 20/20 [00:01<00:00, 10.35it/s]\n",
      "Calculating ROUGE Scores with default: 100%|██████████| 20/20 [00:00<00:00, 25.20it/s]\n"
     ]
    }
   ],
   "source": [
    "df_copy_attacut = df_copy.copy()\n",
    "df_copy_default = df_copy.copy()\n",
    "\n",
    "df_copy_attacut = calculate_update_dataframe_ex(df_copy_attacut, engine='attacut')\n",
    "df_copy_default = calculate_update_dataframe_ex(df_copy_default)  # ใช้ default\n",
    "\n",
    "avg_attacut = df_copy_attacut[[f'ROUGE-1_attacut', f'ROUGE-2_attacut', f'ROUGE-L_attacut', f'ROUGE_Score_attacut']].mean()\n",
    "avg_default = df_copy_default[[f'ROUGE-1_default', f'ROUGE-2_default', f'ROUGE-L_default', f'ROUGE_Score_default']].mean()\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE_Score'],\n",
    "    'Average_Attacut': avg_attacut.values,\n",
    "    'Average_Default': avg_default.values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Average_Attacut</th>\n",
       "      <th>Average_Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROUGE-1</td>\n",
       "      <td>0.658012</td>\n",
       "      <td>0.623011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROUGE-2</td>\n",
       "      <td>0.446077</td>\n",
       "      <td>0.431216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROUGE-L</td>\n",
       "      <td>0.629048</td>\n",
       "      <td>0.593634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ROUGE_Score</td>\n",
       "      <td>0.577712</td>\n",
       "      <td>0.549287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Metric  Average_Attacut  Average_Default\n",
       "0      ROUGE-1         0.658012         0.623011\n",
       "1      ROUGE-2         0.446077         0.431216\n",
       "2      ROUGE-L         0.629048         0.593634\n",
       "3  ROUGE_Score         0.577712         0.549287"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
